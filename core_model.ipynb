{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xaiqo Core Model Implementation\n",
    "\n",
    "This notebook implements the core model architecture for the Xaiqo chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.HEADS\n",
    "        self.head_dim = config.EMBED_DIM // config.HEADS\n",
    "        self.qkv = nn.Linear(config.EMBED_DIM, 3 * config.EMBED_DIM)\n",
    "        self.out = nn.Linear(config.EMBED_DIM, config.EMBED_DIM)\n",
    "        self.dropout = nn.Dropout(config.DROPOUT_RATE)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, D = x.shape\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.view(B, L, self.num_heads, self.head_dim).transpose(1, 2), qkv)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, D)\n",
    "        return self.out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.norm1 = nn.LayerNorm(config.EMBED_DIM)\n",
    "        self.norm2 = nn.LayerNorm(config.EMBED_DIM)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(config.EMBED_DIM, config.HIDDEN_SIZE),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.HIDDEN_SIZE, config.EMBED_DIM),\n",
    "            nn.Dropout(config.DROPOUT_RATE)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attention(self.norm1(x), mask)\n",
    "        x = x + self.feed_forward(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class XaiqoModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.BPE_VOCAB_SIZE, config.EMBED_DIM)\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, config.MAX_SEQ_LENGTH, config.EMBED_DIM))\n",
    "        self.dropout = nn.Dropout(config.DROPOUT_RATE)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.NUM_BLOCKS)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(config.EMBED_DIM)\n",
    "        self.head = nn.Linear(config.EMBED_DIM, config.BPE_VOCAB_SIZE, bias=False)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        B, L = input_ids.shape\n",
    "        \n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = x + self.position_embedding[:, :L, :]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, attention_mask)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}