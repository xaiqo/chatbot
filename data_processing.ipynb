{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing for Xaiqo Chatbot\n",
    "\n",
    "This notebook processes PDF documents into training data for the chatbot model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "%pip install PyPDF2>=3.0.0 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import PyPDF2\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs('/content/data/pdf_documents', exist_ok=True)\n",
    "os.makedirs('/content/data/processed_data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text, chunk_size=512, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        if end < len(text):\n",
    "            # Try to end at a sentence boundary\n",
    "            last_period = text.rfind('.', start, end)\n",
    "            if last_period > start:\n",
    "                end = last_period + 1\n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "def create_training_pairs(chunks):\n",
    "    \"\"\"Create training pairs from text chunks.\"\"\"\n",
    "    training_pairs = []\n",
    "    for chunk in chunks:\n",
    "        # Create direct QA pair\n",
    "        qa_pair = {\n",
    "            'input': f\"Please explain this text: {chunk}\",\n",
    "            'output': chunk\n",
    "        }\n",
    "        training_pairs.append(qa_pair)\n",
    "        \n",
    "        # Create summary pair\n",
    "        sentences = re.split(r'(?<=[.!?]) +', chunk)\n",
    "        if len(sentences) > 2:\n",
    "            summary = ' '.join(sentences[:2]) + '...'\n",
    "            summary_pair = {\n",
    "                'input': f\"Summarize this text: {chunk}\",\n",
    "                'output': summary\n",
    "            }\n",
    "            training_pairs.append(summary_pair)\n",
    "    \n",
    "    return training_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_directory(pdf_dir):\n",
    "    \"\"\"Process all PDFs in directory and create training data.\"\"\"\n",
    "    all_training_pairs = []\n",
    "    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:\n",
    "            chunks = chunk_text(text)\n",
    "            training_pairs = create_training_pairs(chunks)\n",
    "            all_training_pairs.extend(training_pairs)\n",
    "    \n",
    "    return all_training_pairs\n",
    "\n",
    "# Process PDFs and save training data\n",
    "training_pairs = process_pdf_directory('/content/data/pdf_documents')\n",
    "output_path = '/content/data/processed_data/training_data.json'\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_pairs, f, indent=2)\n",
    "\n",
    "print(f\"Created {len(training_pairs)} training pairs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
