{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Xaiqo Chatbot Training on PDF Documents\n",
        "\n",
        "This notebook trains the Xaiqo chatbot model on PDF documents from the `/data/pdf_documents` directory. It extracts text from PDFs, processes the data, and fine-tunes a GPT-2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-dependencies"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "%pip install transformers==4.30.2 torch==2.0.1 PyPDF2==3.0.1 datasets==2.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check-gpu"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to save the trained model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directories for data\n",
        "!mkdir -p /content/data/pdf_documents\n",
        "!mkdir -p /content/data/processed_data\n",
        "!mkdir -p /content/data/extracted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload-pdfs"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Upload your PDF files\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename, content in uploaded.items():\n",
        "    if filename.lower().endswith('.pdf'):\n",
        "        with open(f'/content/data/pdf_documents/{filename}', 'wb') as f:\n",
        "            f.write(content)\n",
        "        print(f\"Saved {filename} to /content/data/pdf_documents/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdf-extraction"
      },
      "outputs": [],
      "source": [
        "# PDF Text Extraction\n",
        "import os\n",
        "import PyPDF2\n",
        "import re\n",
        "import json\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    \n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            \n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text += page.extract_text() + \"\\n\\n\"\n",
        "        \n",
        "        # Clean the text\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "        text = text.strip()\n",
        "        \n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def process_all_pdfs(pdf_dir, output_dir):\n",
        "    \"\"\"Process all PDFs in a directory and save the extracted text.\"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Get all PDF files\n",
        "    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]\n",
        "    \n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in {pdf_dir}\")\n",
        "        return []\n",
        "    \n",
        "    extracted_texts = []\n",
        "    \n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
        "        output_file = os.path.join(output_dir, pdf_file.replace('.pdf', '.txt'))\n",
        "        \n",
        "        # Extract text from PDF\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        \n",
        "        if text:\n",
        "            # Save to text file\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(text)\n",
        "            \n",
        "            extracted_texts.append(text)\n",
        "            print(f\"Processed {pdf_file} -> {output_file}\")\n",
        "        else:\n",
        "            print(f\"Failed to process {pdf_file}\")\n",
        "    \n",
        "    return extracted_texts\n",
        "\n",
        "# Process all PDFs in the directory\n",
        "pdf_dir = '/content/data/pdf_documents'\n",
        "output_dir = '/content/data/extracted_text'\n",
        "extracted_texts = process_all_pdfs(pdf_dir, output_dir)\n",
        "\n",
        "print(f\"Extracted text from {len(extracted_texts)} PDF documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare-training-data"
      },
      "outputs": [],
      "source": [
        "# Prepare training data in the format expected by the model\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "def chunk_text(text, chunk_size=1000, overlap=200):\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        \n",
        "        # Try to end at a sentence boundary\n",
        "        if end < len(text):\n",
        "            # Look for sentence end within the last 100 characters\n",
        "            search_end = max(end - 100, start)\n",
        "            sentence_end = text.rfind('. ', search_end, end)\n",
        "            if sentence_end > search_end:\n",
        "                end = sentence_end + 1\n",
        "        \n",
        "        chunks.append(text[start:end])\n",
        "        start = end - overlap\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "def create_qa_pairs(chunks):\n",
        "    \"\"\"Create question-answer pairs from text chunks for training.\"\"\"\n",
        "    qa_pairs = []\n",
        "    \n",
        "    for i, chunk in enumerate(chunks):\n",
        "        # Create a simple QA pair for each chunk\n",
        "        qa_pair = {\n",
        "            'user': f\"What information can you provide about this text: {chunk[:100]}...\",\n",
        "            'bot': chunk\n",
        "        }\n",
        "        qa_pairs.append(qa_pair)\n",
        "        \n",
        "        # Create some more specific questions based on content\n",
        "        # This is a simple approach - in a real scenario, you might want to use\n",
        "        # more sophisticated NLP techniques to generate better questions\n",
        "        sentences = re.split(r'(?<=[.!?]) +', chunk)\n",
        "        if len(sentences) > 3:\n",
        "            summary = ' '.join(sentences[:3]) + '...'\n",
        "            qa_pair = {\n",
        "                'user': f\"Can you summarize this information?\",\n",
        "                'bot': f\"Here's a summary: {summary}\"\n",
        "            }\n",
        "            qa_pairs.append(qa_pair)\n",
        "    \n",
        "    return qa_pairs\n",
        "\n",
        "# Process all extracted texts\n",
        "all_qa_pairs = []\n",
        "\n",
        "for text in extracted_texts:\n",
        "    chunks = chunk_text(text)\n",
        "    qa_pairs = create_qa_pairs(chunks)\n",
        "    all_qa_pairs.extend(qa_pairs)\n",
        "\n",
        "# Save the training data\n",
        "training_data_path = '/content/data/processed_data/train.json'\n",
        "os.makedirs(os.path.dirname(training_data_path), exist_ok=True)\n",
        "\n",
        "with open(training_data_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_qa_pairs, f, indent=2)\n",
        "\n",
        "print(f\"Created {len(all_qa_pairs)} training examples from PDF content\")\n",
        "print(f\"Training data saved to {training_data_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-model"
      },
      "outputs": [],
      "source": [
        "# Train the model on the processed data\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    GPT2Tokenizer, \n",
        "    GPT2LMHeadModel, \n",
        "    AdamW, \n",
        "    get_linear_schedule_with_warmup,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ConversationDataset(Dataset):\n",
        "    \"\"\"Dataset for training the chatbot on conversation data\"\"\"\n",
        "    def __init__(self, data_path, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.examples = []\n",
        "        \n",
        "        # Load data from file\n",
        "        if os.path.exists(data_path):\n",
        "            with open(data_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                \n",
        "            # Process conversations\n",
        "            for conversation in data:\n",
        "                if isinstance(conversation, list):\n",
        "                    # Format: list of message pairs\n",
        "                    formatted_text = self.tokenizer.bos_token\n",
        "                    for user_msg, bot_msg in conversation:\n",
        "                        formatted_text += f\"User: {user_msg}{self.tokenizer.sep_token}\"\n",
        "                        formatted_text += f\"Bot: {bot_msg}{self.tokenizer.sep_token}\"\n",
        "                    \n",
        "                    self.examples.append(formatted_text)\n",
        "                elif isinstance(conversation, dict) and 'user' in conversation and 'bot' in conversation:\n",
        "                    # Format: dict with 'user' and 'bot' keys\n",
        "                    formatted_text = self.tokenizer.bos_token\n",
        "                    formatted_text += f\"User: {conversation['user']}{self.tokenizer.sep_token}\"\n",
        "                    formatted_text += f\"Bot: {conversation['bot']}{self.tokenizer.sep_token}\"\n",
        "                    \n",
        "                    self.examples.append(formatted_text)\n",
        "        else:\n",
        "            logger.warning(f\"Data file {data_path} not found. Using empty dataset.\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.examples[idx]\n",
        "        encodings = self.tokenizer(text, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
        "        \n",
        "        # Create labels (same as input_ids for language modeling)\n",
        "        encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
        "        \n",
        "        # Convert to tensors\n",
        "        item = {key: torch.tensor(val) for key, val in encodings.items()}\n",
        "        return item\n",
        "\n",
        "# Define training parameters\n",
        "output_dir = '/content/drive/MyDrive/xaiqo_models'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f'{output_dir}/results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=f'{output_dir}/logs',\n",
        "    logging_steps=10,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU is available\n",
        ")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Add special tokens\n",
        "special_tokens = {\n",
        "    'pad_token': '<PAD>',\n",
        "    'bos_token': '<BOS>',\n",
        "    'eos_token': '<EOS>',\n",
        "    'sep_token': '<SEP>'\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Load your dataset\n",
        "train_dataset = ConversationDataset('/content/data/processed_data/train.json', tokenizer)\n",
        "\n",
        "# Split into train and validation if dataset is large enough\n",
        "if len(train_dataset) > 10:  # Only split if we have enough examples\n",
        "    train_size = int(0.9 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "else:\n",
        "    val_dataset = None\n",
        "    training_args.evaluation_strategy = \"no\"  # Disable evaluation if no validation set\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model_save_path = f'{output_dir}/final_model'\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Model successfully trained and saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test-model"
      },
      "outputs": [],
      "source": [
        "# Test the trained model\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "class ImprovedChatbot:\n",
        "    \"\"\"A simplified version of the chatbot for testing\"\"\"\n",
        "    def __init__(self, model_path):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        \n",
        "        # Load tokenizer and model\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_path).to(self.device)\n",
        "        self.model.eval()\n",
        "        \n",
        "    def answer_question(self, question, max_length=100):\n",
        "        # Format input\n",
        "        input_text = f\"{self.tokenizer.bos_token}User: {question}{self.tokenizer.sep_token}Bot:\"\n",
        "        \n",
        "        # Encode input\n",
        "        input_ids = self.tokenizer.encode(input_text, return_tensors='pt').to(self.device)\n",
        "        \n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            output = self.model.generate(\n",
        "                input_ids,\n",
        "                max_length=input_ids.shape[1] + max_length,\n",
        "                temperature=0.7,\n",
        "                top_k=50,\n",
        "                top_p=0.95,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=3,\n",
        "                repetition_penalty=1.2,\n",
        "                num_return_sequences=1\n",
        "            )\n",
        "        \n",
        "        # Decode response\n",
        "        response = self.tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "        \n",
        "        # Clean up response\n",
        "        response = response.strip()\n",
        "        if \"User:\" in response:\n",
        "            response = response.split(\"User:\")[0].strip()\n",
        "            \n",
        "        return response\n",
        "\n",
        "# Initialize chatbot with the trained model\n",
        "model_path = '/content/drive/MyDrive/xaiqo_models/final_model'\n",
        "chatbot = ImprovedChatbot(model_path)\n",
        "\n",
        "test_questions = [\n",
        "    \"What information can you provide based on the documents you were trained on?\",\n",
        "    \"Can you summarize the key points from the documents?\",\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "    response = chatbot.answer_question(question)\n",
        "    print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook has demonstrated how to:\n",
        "\n",
        "1. Extract text from PDF documents\n",
        "2. Process the extracted text into a format suitable for training\n",
        "3. Fine-tune a GPT-2 model on the processed data\n",
        "4. Test the trained model with sample questions\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Xaiqo_PDF_Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
